{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jko/ice3d')\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from data.single_view_dataset import SingleViewDataset\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from models.resnet18_regression import ResNet18Regression\n",
    "from models.resnet18_classification import ResNet18Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torchvision.models import ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "n_rand = 666 # random seed\n",
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "# Load the class mapping from a JSON file\n",
    "class_mapping_file = '/home/jko/ice3d/data/class_to_idx.json'\n",
    "# Load class mapping from JSON file\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "num_classes = len(class_to_idx)  # Number of unique classes in n_arms\n",
    "# # set indices of train/val/test sets\n",
    "split = [0.7, 0.15, 0.15]\n",
    "n_data = 70_000\n",
    "assert abs(sum(split) - 1.0) < 1e-8, \"Split does not sum to 1\"\n",
    "n_train = int(split[0] * n_data)\n",
    "n_val = int(split[1] * n_data)\n",
    "n_test = n_data - n_train - n_val  # ensures all data is used\n",
    "train_idx = list(range(0, n_train))\n",
    "val_idx = list(range(n_train, n_train + n_val))\n",
    "test_idx = list(range(n_train + n_val, n_data))\n",
    "# define log transform for later\n",
    "def log_transform(x):\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(data_type, input_channels, task_type):\n",
    "    transforms = {}\n",
    "    # Define transforms based on data_type\n",
    "    if data_type in ['single_view_h5', 'stereo_view_h5']:\n",
    "        train_transform = T.Compose([\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.Normalize(mean=[0.5] * input_channels, std=[1.0] * input_channels)\n",
    "            ])\n",
    "        val_transform = T.Compose([\n",
    "                T.Normalize(mean=[0.5] * input_channels, std=[1.0] * input_channels)\n",
    "            ])\n",
    "        transforms['train'] = train_transform\n",
    "        transforms['val'] = val_transform\n",
    "        transforms['test'] = val_transform\n",
    "        # define target transform\n",
    "        if task_type == 'classification':\n",
    "            target_transform = None\n",
    "        else:\n",
    "            def log_transform(x):\n",
    "                return torch.log(x)\n",
    "            target_transform = log_transform\n",
    "        transforms['train_target'] = target_transform\n",
    "        transforms['val_target'] = target_transform\n",
    "        transforms['test_target'] = target_transform    \n",
    "        return transforms\n",
    "    elif data_type == 'tabular':\n",
    "        # define target transform\n",
    "        if task_type == 'classification':\n",
    "            target_transform = None\n",
    "        else:\n",
    "            def log_transform(x):\n",
    "                return torch.log(x)\n",
    "            target_transform = log_transform\n",
    "        transforms['target'] = target_transform\n",
    "        return transforms\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single view (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small'\n",
    "data_file = 'default_shuffled_small.h5'\n",
    "data_path = os.path.join(data_dir, data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['rho_eff', 'sa_eff']\n",
    "data_type = 'single_view_h5'\n",
    "input_channels = 1\n",
    "task_type = 'regression'\n",
    "transforms = get_transforms(data_type, input_channels, task_type)\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=data_path,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=None,\n",
    "    subset_seed=n_rand,\n",
    "    num_workers=16,\n",
    "    prefetch_factor=16,\n",
    "    train_transform=transforms['train'],\n",
    "    val_transform=transforms['val'],\n",
    "    test_transform=transforms['test'],\n",
    "    train_target_transform=transforms['train_target'],\n",
    "    val_target_transform=transforms['val_target'],\n",
    "    test_target_transform=transforms['test_target'],\n",
    "    task_type='regression',\n",
    "    class_to_idx=None\n",
    ")\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "input_channels = 1 \n",
    "output_size = len(targets)\n",
    "model = ResNet18Regression(\n",
    "    input_channels=1, \n",
    "    output_size=2, \n",
    "    learning_rate=1e-3, \n",
    "    weights=ResNet18_Weights.DEFAULT)\n",
    "# Set up logger information\n",
    "log_dir = '/home/jko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'resnet18-regression-subset-700k-tb'\n",
    "csv_log_name = f'resnet18-regression-subset-700k-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | resnet | ResNet | 11.2 M | train\n",
      "------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.685    Total estimated model params size (MB)\n",
      "68        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 382/382 [00:22<00:00, 16.95it/s, v_num=1]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 382/382 [00:20<00:00, 18.59it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.014. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 382/382 [00:21<00:00, 18.01it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "# set up early stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',       # Metric name to monitor\n",
    "    patience=5,               # Stop after 5 epochs of no improvement\n",
    "    verbose=True,\n",
    "    min_delta=0.01,\n",
    "    mode='min'                # 'min' for loss, 'max' for accuracy\n",
    ")\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "# Train the model\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "log_path = '/home/jko/ice3d/models/lightning_logs/resnet18-regression-subset-700k-csv/version_0/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo View (2DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small'\n",
    "data_file1 = 'default_shuffled_small.h5'\n",
    "data_file2 = '2ds_shuffled_small.h5'\n",
    "data_path1 = os.path.join(data_dir, data_file1)\n",
    "data_path2 = os.path.join(data_dir, data_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['rho_eff', 'sa_eff']\n",
    "data_type = 'single_view_h5'\n",
    "input_channels = 2\n",
    "task_type = 'regression'\n",
    "transforms = get_transforms(data_type, input_channels, task_type)\n",
    "dm = StereoViewDataModule(\n",
    "    hdf_file_left=data_path1,\n",
    "    hdf_file_right=data_path2,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=None,\n",
    "    subset_seed=n_rand,\n",
    "    num_workers=16,\n",
    "    prefetch_factor=16,\n",
    "    train_transform=transforms['train'],\n",
    "    val_transform=transforms['val'],\n",
    "    test_transform=transforms['test'],\n",
    "    train_target_transform=transforms['train_target'],\n",
    "    val_target_transform=transforms['val_target'],\n",
    "    test_target_transform=transforms['test_target'],\n",
    "    task_type='regression',\n",
    "    class_to_idx=None\n",
    ")\n",
    "dm.setup()\n",
    "# instantiate model\n",
    "model = ResNet18Regression(\n",
    "    input_channels=input_channels, \n",
    "    output_size=len(targets), \n",
    "    learning_rate=lr, \n",
    "    pretrained=True)\n",
    "# Set up logger information\n",
    "log_dir = '/home/jko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'resnet18-regression-stereo-2ds-subset-700k-tb'\n",
    "csv_log_name = f'resnet18-regression-stereo-2dssubset-700k-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "# Train the model\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "log_path = '/home/jko/ice3d/models/lightning_logs/resnet18-regression-stereo-2ds-subset-700k-csv/version_0/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.3 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8658764d9b797a2c8f9923ddcd38c86560d2e4c4233111378203e5da49e50175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
