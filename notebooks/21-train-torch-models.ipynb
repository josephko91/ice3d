{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86477af-3c28-415d-a7e9-6bd3bddaa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06caab21-4f42-4b70-8ead-5ac816f22114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "Is GPU Available: True\n",
      "GPU Name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(\"Is GPU Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61129177-9d33-4e1a-9ae5-30e6c9d8e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# # from data_loading import RegressionTaskData\n",
    "# # from network import CNNRegression\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import shutil\n",
    "\n",
    "# from typing import Tuple\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import torchvision\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52da26b3-35ea-4cca-87aa-ad82f7b6c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "class RosetteDataSet(Dataset):\n",
    "    def __init__(self, image_dir, labels_file, split='train', transform=None, target='n_arms'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Directory with subfolders ('train', 'val', 'test') containing images.\n",
    "            csv_file (string): CSV file containing image filenames and labels.\n",
    "            split (string): Which data split to use ('train', 'val', 'test').\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            target (string): ['n_arms', 'rho_eff', 'sa_eff']\n",
    "        \"\"\"\n",
    "        self.image_dir = os.path.join(image_dir, split)  # Select the correct subfolder (train/val/test)\n",
    "        self.labels = pd.read_csv(labels_file)\n",
    "        self.labels = self.labels[self.labels['split'] == split].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Load an image and its corresponding label(s)\"\"\"\n",
    "        row = self.labels.iloc[idx]\n",
    "        img_name = row['filename']  # Get the image filename\n",
    "        img_path = os.path.join(self.image_dir, img_name)  # Construct the full image path\n",
    "        image = Image.open(img_path)\n",
    "        # Classification label (e.g. n_arms)\n",
    "        n_arms_labels = torch.tensor(row['n_arms'], dtype=torch.long)\n",
    "        # Regression targets\n",
    "        regression_targets = torch.tensor([row['rho_eff'], row['sa_eff']], dtype=torch.float32)\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, n_arms_labels, regression_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0b0248-a025-4210-9797-cb64d1a81434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>n_arms</th>\n",
       "      <th>rho_eff</th>\n",
       "      <th>sa_eff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ros-projection-010913-002-default.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.166602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ros-projection-016265-058-default.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.324473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ros-projection-001142-087-default.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>0.147579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ros-projection-017988-092-default.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100758</td>\n",
       "      <td>0.443553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ros-projection-057113-091-default.png</td>\n",
       "      <td>9</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.280913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  n_arms   rho_eff    sa_eff\n",
       "0  ros-projection-010913-002-default.png       5  0.015800  0.166602\n",
       "1  ros-projection-016265-058-default.png       5  0.057668  0.324473\n",
       "2  ros-projection-001142-087-default.png       4  0.014152  0.147579\n",
       "3  ros-projection-017988-092-default.png       5  0.100758  0.443553\n",
       "4  ros-projection-057113-091-default.png       9  0.026300  0.280913"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split/labels.csv'\n",
    "labels = pd.read_csv(labels_file)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8b2c17a-62e0-409f-b0ff-393f4270db25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fa46b-0c14-4cba-8334-30548509936f",
   "metadata": {},
   "source": [
    "# RosetteDataModule: handles loading/splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69b34e5-8dd2-405f-bf8f-684ff87b7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RosetteDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, image_dir, labels_file, batch_size, target, transform=None):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.batch_size = batch_size\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = RosetteDataSet(self.image_dir, self.labels_file, split='train', transform=self.transform, target=self.target)\n",
    "        self.val_dataset = RosetteDataSet(self.image_dir, self.labels_file, split='val', transform=self.transform, target=self.target)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4c458-acb9-401a-910b-bb76b1568272",
   "metadata": {},
   "source": [
    "# RosetteModel: LightningModule for one task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02384f4f-ae9c-48ae-8585-02e4bd77a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RosetteModel(pl.LightningModule):\n",
    "    def __init__(self, task='classification', num_classes=5, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.task = task\n",
    "        self.lr = lr\n",
    "\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 1 if task == 'regression' else num_classes)\n",
    "\n",
    "        if task == 'classification':\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "        elif task == 'regression':\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, class_label, reg_target = batch\n",
    "\n",
    "        if self.task == 'classification':\n",
    "            y_hat = self(x)\n",
    "            loss = self.loss_fn(y_hat, class_label)\n",
    "        elif self.task == 'regression':\n",
    "            y_hat = self(x)\n",
    "            # Choose target from regression vector\n",
    "            target_idx = 0 if self.hparams.target == \"rho_eff\" else 1\n",
    "            y = reg_target[:, target_idx].unsqueeze(1)\n",
    "            loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.training_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f4b2c-7abb-4cd7-acb3-e89f46f94beb",
   "metadata": {},
   "source": [
    "# Run training for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b548986-08e0-466b-846d-06f5dafe9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8d861-c287-4fb5-83c3-ac4642b462a6",
   "metadata": {},
   "source": [
    "## a) Predict n_arms (classificaion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa8a746-1894-4a34-9d57-a34452367f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>n_arms</th>\n",
       "      <th>rho_eff</th>\n",
       "      <th>sa_eff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ros-projection-010913-002-default.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.166602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ros-projection-016265-058-default.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.324473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ros-projection-001142-087-default.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>0.147579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ros-projection-017988-092-default.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100758</td>\n",
       "      <td>0.443553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ros-projection-057113-091-default.png</td>\n",
       "      <td>9</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.280913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  n_arms   rho_eff    sa_eff\n",
       "0  ros-projection-010913-002-default.png       5  0.015800  0.166602\n",
       "1  ros-projection-016265-058-default.png       5  0.057668  0.324473\n",
       "2  ros-projection-001142-087-default.png       4  0.014152  0.147579\n",
       "3  ros-projection-017988-092-default.png       5  0.100758  0.443553\n",
       "4  ros-projection-057113-091-default.png       9  0.026300  0.280913"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf49dbfc-3e1c-4cd7-b9c5-320b594c6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels['n_arms'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69fad645-a1c0-43c9-9af3-bf6c6124c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/joko/.conda/envs/torch/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/glade/u/home/joko/.conda/envs/torch/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-462d23b3-6316-58a3-a82d-26d5032a13c6]\n",
      "\n",
      "  | Name     | Type             | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | backbone | ResNet           | 11.2 M | train\n",
      "1 | loss_fn  | CrossEntropyLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.720    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/joko/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split/val/ros-projection-010913-002-default.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m model_class = RosetteModel(task=\u001b[33m'\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m'\u001b[39m, num_classes=num_classes)\n\u001b[32m      5\u001b[39m trainer = Trainer(max_epochs=\u001b[32m10\u001b[39m, accelerator=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:138\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    137\u001b[39m     dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     batch, batch_idx, dataloader_idx = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx != dataloader_idx:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m._store_dataloader_outputs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/loops/fetchers.py:134\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batches\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     batch = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/loops/fetchers.py:61\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._start_profiler()\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _ITERATOR_RETURN:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001b[39m, in \u001b[36m_Sequential.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._use_next_iterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mRosetteDataSet.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     24\u001b[39m img_name = row[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# Get the image filename\u001b[39;00m\n\u001b[32m     25\u001b[39m img_path = os.path.join(\u001b[38;5;28mself\u001b[39m.image_dir, img_name)  \u001b[38;5;66;03m# Construct the full image path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Classification label (e.g. n_arms)\u001b[39;00m\n\u001b[32m     28\u001b[39m n_arms_labels = torch.tensor(row[\u001b[33m'\u001b[39m\u001b[33mn_arms\u001b[39m\u001b[33m'\u001b[39m], dtype=torch.long)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.13/site-packages/PIL/Image.py:3465\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3462\u001b[39m     filename = os.fspath(fp)\n\u001b[32m   3464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3465\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3466\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3467\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split/val/ros-projection-010913-002-default.png'"
     ]
    }
   ],
   "source": [
    "data_dir = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split'\n",
    "num_classes = len(labels['n_arms'].unique())\n",
    "dm_class = RosetteDataModule(data_dir, labels_file, batch_size=64, target=\"n_arms\", transform=transform)\n",
    "model_class = RosetteModel(task='classification', num_classes=num_classes)\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(model_class, dm_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3c9b7-51f1-426e-bd40-22d9fae1a5a0",
   "metadata": {},
   "source": [
    "## b) Predict rho_eff (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ae9b2-b77f-4fa3-9fef-ec95bd49e6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12299a80-3456-47ed-927a-29ea1de8412f",
   "metadata": {},
   "source": [
    "## c) Predict sa_eff (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa0ef5-624a-463b-a3d6-1b6dfe5241b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec0821-8d34-44c4-a992-256697b11876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468bc01-b99b-4db3-863d-afb18cece7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch]",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
