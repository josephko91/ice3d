{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Joseph Ko <br>\n",
    "Reproducible notebook to train pytorch models from: \"A Machine Learning Framework for Predicting Microphysical Properties of Ice Crystals from Cloud Particle Imagery\" (Ko et al. 2025) <br>\n",
    "Required packages: see torch.yaml file for required files\n",
    "Models were trained using NVIDIA a100 GPUs. Package configurations may vary depending on the GPU you use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86477af-3c28-415d-a7e9-6bd3bddaa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "# Add your project root to sys.path for imports \n",
    "# (models and data modules should be in this directory)\n",
    "sys.path.append('/home/jko/ice3d')\n",
    "# Import your models and datamodules\n",
    "from models.mlp_regression import MLPRegression\n",
    "from models.mlp_classification import MLPClassification\n",
    "from models.cnn_regression import VanillaCNNRegression\n",
    "from models.cnn_classification import VanillaCNNClassification\n",
    "from models.resnet18_regression import ResNet18Regression\n",
    "from models.resnet18_classification import ResNet18Classification\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from data.stereo_view_datamodule import StereoViewDataModule\n",
    "from data.tabular_datamodule import TabularDataModule\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configurations and global paths\n",
    "targets_reg = 'rho_eff,sa_eff'\n",
    "targets_cls = 'n_arms'\n",
    "tabular_path = '/home/jko/synth-ros-data/tabular-data-v2/shuffled_small/ros-tabular-data-shuffled-default-subset-700000.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-configure argument lists for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the args of each model in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {} # initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_reg = 'rho_eff,sa_eff'\n",
    "tabular_path = '/home/jko/synth-ros-data/tabular-data-v2/shuffled_small/ros-tabular-data-shuffled-default-subset-700000.parquet'\n",
    "args = SimpleNamespace(\n",
    "    model='mlp_regression',\n",
    "    data_type='tabular',\n",
    "    tabular_file=tabular_path,\n",
    "    # targets='rho_eff,sa_eff',\n",
    "    targets=targets_reg,\n",
    "    input_channels=3,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    max_epochs=10,\n",
    "    subset_size=1.0,\n",
    "    seed=42,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    task_type='regression',\n",
    "    log_dir='./lightning_logs',\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    class_to_idx_json=None,\n",
    "    feature_names=None,\n",
    "    num_gpus=1,\n",
    "    train_idx=None,\n",
    "    val_idx=None,\n",
    "    test_idx=None\n",
    ")\n",
    "args_dict['mlp_reg'] = args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    model='mlp_regression',\n",
    "    data_type='single_view_h5',\n",
    "    hdf_file='path/to/file.h5',\n",
    "    hdf_file_left=None,\n",
    "    hdf_file_right=None,\n",
    "    tabular_file=None,\n",
    "    targets='rho_eff,sa_eff',\n",
    "    input_channels=3,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    max_epochs=10,\n",
    "    subset_size=1.0,\n",
    "    seed=42,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    task_type='regression',\n",
    "    log_dir='./lightning_logs',\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    class_to_idx_json=None,\n",
    "    feature_names=None,\n",
    "    num_gpus=1,\n",
    "    train_idx=None,\n",
    "    val_idx=None,\n",
    "    test_idx=None\n",
    ")\n",
    "args_dict['mlp_cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args, input_size=None, output_size=None, num_classes=None):\n",
    "    if args.model == 'mlp_regression':\n",
    "        return MLPRegression(input_size, output_size, learning_rate=args.lr)\n",
    "    elif args.model == 'mlp_classification':\n",
    "        return MLPClassification(input_size, num_classes, learning_rate=args.lr)\n",
    "    elif args.model == 'cnn_regression':\n",
    "        return VanillaCNNRegression(input_channels=args.input_channels, output_size=output_size, learning_rate=args.lr)\n",
    "    elif args.model == 'cnn_classification':\n",
    "        return VanillaCNNClassification(input_channels=args.input_channels, num_classes=num_classes, learning_rate=args.lr)\n",
    "    elif args.model == 'resnet18_regression':\n",
    "        return ResNet18Regression(input_channels=args.input_channels, output_size=output_size, learning_rate=args.lr)\n",
    "    elif args.model == 'resnet18_classification':\n",
    "        return ResNet18Classification(input_channels=args.input_channels, num_classes=num_classes, learning_rate=args.lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model}\")\n",
    "\n",
    "def get_transforms(args):\n",
    "    transforms = {}\n",
    "    # Define transforms based on data_type\n",
    "    if args.data_type in ['single_view_h5', 'stereo_view_h5']:\n",
    "        train_transform = T.Compose([\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.Normalize(mean=[0.5] * args.input_channels, std=[1.0] * args.input_channels)\n",
    "            ])\n",
    "        val_transform = T.Compose([\n",
    "                T.Normalize(mean=[0.5] * args.input_channels, std=[1.0] * args.input_channels)\n",
    "            ])\n",
    "        transforms['train'] = train_transform\n",
    "        transforms['val'] = val_transform\n",
    "        transforms['test'] = val_transform\n",
    "        # define target transform\n",
    "        if args.task_type == 'classification':\n",
    "            target_transform = None\n",
    "        else:\n",
    "            def log_transform(x):\n",
    "                return torch.log(x)\n",
    "            target_transform = log_transform\n",
    "        transforms['train_target'] = target_transform\n",
    "        transforms['val_target'] = target_transform\n",
    "        transforms['test_target'] = target_transform    \n",
    "        return transforms\n",
    "    elif args.data_type == 'tabular':\n",
    "        # define target transform\n",
    "        if args.task_type == 'classification':\n",
    "            target_transform = None\n",
    "        else:\n",
    "            def log_transform(x):\n",
    "                return torch.log(x)\n",
    "            target_transform = log_transform\n",
    "        transforms['target'] = target_transform\n",
    "        return transforms\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_datamodule(args, class_to_idx=None):\n",
    "    transforms = get_transforms(args)\n",
    "    if args.data_type == 'single_view_h5':\n",
    "        return SingleViewDataModule(\n",
    "            hdf_file=args.hdf_file,\n",
    "            target_names=args.targets.split(','),\n",
    "            train_idx=None,\n",
    "            val_idx=None,\n",
    "            test_idx=None,\n",
    "            batch_size=args.batch_size,\n",
    "            subset_size=args.subset_size,\n",
    "            subset_seed=args.seed,\n",
    "            num_workers=args.num_workers,\n",
    "            prefetch_factor=args.prefetch_factor,\n",
    "            train_transform=transforms['train'],\n",
    "            val_transform=transforms['val'],\n",
    "            test_transform=transforms['test'],\n",
    "            train_target_transform=transforms['train_target'],\n",
    "            val_target_transform=transforms['val_target'],\n",
    "            test_target_transform=transforms['test_target'],\n",
    "            task_type=args.task_type,\n",
    "            class_to_idx=class_to_idx\n",
    "        )\n",
    "    elif args.data_type == 'stereo_view_h5':\n",
    "        return StereoViewDataModule(\n",
    "            hdf_file_left=args.hdf_file_left,\n",
    "            hdf_file_right=args.hdf_file_right,\n",
    "            target_names=args.targets.split(','),\n",
    "            train_idx=None,\n",
    "            val_idx=None,\n",
    "            test_idx=None,\n",
    "            batch_size=args.batch_size,\n",
    "            subset_size=args.subset_size,\n",
    "            subset_seed=args.seed,\n",
    "            num_workers=args.num_workers,\n",
    "            prefetch_factor=args.prefetch_factor,\n",
    "            train_transform=transforms['train'],\n",
    "            val_transform=transforms['val'],\n",
    "            test_transform=transforms['test'],\n",
    "            train_target_transform=transforms['train_target'],\n",
    "            val_target_transform=transforms['val_target'],\n",
    "            test_target_transform=transforms['test_target'],\n",
    "            task_type=args.task_type,\n",
    "            class_to_idx=class_to_idx\n",
    "        )\n",
    "    elif args.data_type == 'tabular':\n",
    "        feature_names = args.feature_names.split(',') if args.feature_names else None\n",
    "        return TabularDataModule(\n",
    "            data_file=args.tabular_file,\n",
    "            feature_names=feature_names,\n",
    "            target_names=args.targets.split(','),\n",
    "            batch_size=args.batch_size,\n",
    "            subset_size=args.subset_size,\n",
    "            subset_seed=args.seed,\n",
    "            num_workers=args.num_workers,\n",
    "            task_type=args.task_type,\n",
    "            class_to_idx=class_to_idx,\n",
    "            target_transform=transforms['target'],\n",
    "            train_idx=args.train_idx,\n",
    "            val_idx=args.val_idx,   \n",
    "            test_idx=args.test_idx\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data type: {args.data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "953f4b2c-7abb-4cd7-acb3-e89f46f94beb",
   "metadata": {},
   "source": [
    "# Run training for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8d861-c287-4fb5-83c3-ac4642b462a6",
   "metadata": {},
   "source": [
    "## a) Predict n_arms (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd2264-9b74-496f-99de-fc4491762f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer \n",
    "data_dir = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split'\n",
    "num_classes = len(labels['n_arms'].unique())\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = 'resnet-18-n1000-default-n_arms-tb'\n",
    "csv_log_name = 'resnet-18-n1000-default-n_arms-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dm_class = RosetteDataModule(data_dir, labels_file, batch_size=32, target=\"n_arms\", transform=transform)\n",
    "model_class = RosetteModel(task='classification', num_classes=num_classes, lr=2e-7)\n",
    "trainer = Trainer(max_epochs=1000, \n",
    "                  accelerator=\"auto\",\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fad645-a1c0-43c9-9af3-bf6c6124c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.fit(model_class, dm_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8857494-4c9e-484e-a358-874b359caad0",
   "metadata": {},
   "source": [
    "Plot validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df64b29-5cd2-4f09-8c59-39080a9c954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/resnet-18-n1000-default-n_arms-csv/version_29/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Drop the 'step' column\n",
    "metrics_df = metrics_df.drop(columns=['step'])\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "    'val_acc': 'mean'       # Take the mean of the validation accuracy over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cc996-a39d-43e1-adec-48c5f3d6693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run validation\n",
    "# val_results = trainer.validate(model_class, datamodule=dm_class)\n",
    "# print(val_results)\n",
    "\n",
    "# Extract predictions and targets from the model instance\n",
    "preds = model_class.val_preds\n",
    "targets = model_class.val_targets\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(targets, preds, normalize='true')\n",
    "# Plot the confusion matrix\n",
    "class_labels = ['4', '5', '6', '7', '8', '9', '10']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f212e2-394b-4da5-a536-9665b133cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original class labels as strings\n",
    "# class_labels = ['4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "# # Put model in evaluation mode\n",
    "# model_class.eval()\n",
    "# val_loader = dm_class.val_dataloader()\n",
    "\n",
    "# all_preds = []\n",
    "# all_targets = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in val_loader:\n",
    "#         x, class_label, _ = batch\n",
    "#         x = x.to(model_class.device)\n",
    "#         y_hat = model_class(x)\n",
    "#         preds = torch.argmax(y_hat, dim=1)\n",
    "\n",
    "#         all_preds.extend(preds.cpu().numpy())\n",
    "#         all_targets.extend(class_label.cpu().numpy())\n",
    "\n",
    "# # Compute the confusion matrix (normalized per row)\n",
    "# cm = confusion_matrix(all_targets, all_preds, normalize='true')\n",
    "\n",
    "# # Plot\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "# plt.title(\"Normalized Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457de4ef-0558-4d08-bbaf-2306d5d04fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46b3c9b7-51f1-426e-bd40-22d9fae1a5a0",
   "metadata": {},
   "source": [
    "## b) Predict rho_eff (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185a999-c456-4090-9515-83543d307946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer \n",
    "data_dir = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split'\n",
    "num_classes = len(labels['n_arms'].unique())\n",
    "target = 'rho_eff'\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'resnet-18-n1000-default-{target}-tb'\n",
    "csv_log_name = f'resnet-18-n1000-default-{target}-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dm_class = RosetteDataModule(data_dir, labels_file, batch_size=32, target=target, transform=transform)\n",
    "model_rho = RosetteModel(task='regression', \n",
    "                         num_classes=num_classes, \n",
    "                         lr=1e-6, \n",
    "                         target='rho_eff', \n",
    "                         weights=None)\n",
    "trainer = Trainer(max_epochs=50, \n",
    "                  accelerator=\"auto\",\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a14bf-6ad5-4015-bb68-4c490c684d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.fit(model_rho, dm_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d22f8-b840-4185-992e-6ed5482f35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/resnet-18-n1000-default-rho_eff-csv/version_5/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Drop the 'step' column\n",
    "metrics_df = metrics_df.drop(columns=['step'])\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "    'val_mae': 'mean'       # Take the mean of the validation accuracy over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493d455-c755-48c3-8a56-6818706ecfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation scatter plot\n",
    "# Extract predictions and targets from the model instance\n",
    "preds = model_rho.val_preds\n",
    "targets = model_rho.val_targets\n",
    "# R2 value\n",
    "r2 = r2_score(preds, targets)\n",
    "print(f'R2 value for effective density: {r2}')\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Create hexbin plot\n",
    "hb = ax.hexbin(preds, targets, gridsize=30, \n",
    "               cmap='viridis', mincnt=1, alpha=0.9, bins='log')\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax)\n",
    "cbar.set_label('Log(Counts)')\n",
    "# Plot the identity line (the line where predicted equals truth)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.axline(xy1=(0, 0), slope=1, color='black')\n",
    "# Set axis limits and labels\n",
    "ax.set_xlim(lims[0], lims[1])\n",
    "ax.set_ylim(lims[0], lims[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_title('Effective Density [unitless]')\n",
    "# Add R2 text annotation in relative figure coordinates\n",
    "ax.text(0.25, 0.75, f'$R^2$ = {r2:.2f}', transform=plt.gcf().transFigure, ha='left', size=16)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12299a80-3456-47ed-927a-29ea1de8412f",
   "metadata": {},
   "source": [
    "## c) Predict sa_eff (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa0ef5-624a-463b-a3d6-1b6dfe5241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer \n",
    "data_dir = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/subset_n1000_default_split'\n",
    "num_classes = len(labels['n_arms'].unique())\n",
    "target = 'sa_eff'\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'resnet-18-n1000-default-{target}-tb'\n",
    "csv_log_name = f'resnet-18-n1000-default-{target}-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dm_class = RosetteDataModule(data_dir, labels_file, batch_size=16, target=target, transform=transform)\n",
    "model_rho = RosetteModel(task='regression', \n",
    "                         num_classes=num_classes, \n",
    "                         lr=5e-7, \n",
    "                         target=target, \n",
    "                         weights=None)\n",
    "trainer = Trainer(max_epochs=500, \n",
    "                  accelerator=\"auto\",\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7e6b6-0bf4-4e4f-918f-d168c6bacc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.fit(model_rho, dm_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9432a32-20e5-4cbe-95e3-ec76211a6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = f'/glade/u/home/joko/ice3d/models/lightning_logs/resnet-18-n1000-default-{target}-csv/version_4/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Drop the 'step' column\n",
    "metrics_df = metrics_df.drop(columns=['step'])\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "    'val_mae': 'mean'       # Take the mean of the validation accuracy over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec0821-8d34-44c4-a992-256697b11876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation scatter plot\n",
    "# Extract predictions and targets from the model instance\n",
    "preds = model_rho.val_preds\n",
    "targets = model_rho.val_targets\n",
    "# R2 value\n",
    "r2 = r2_score(preds, targets)\n",
    "print(f'R2 value for effective density: {r2}')\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Create hexbin plot\n",
    "hb = ax.hexbin(preds, targets, gridsize=30, \n",
    "               cmap='viridis', mincnt=1, alpha=0.9, bins='log')\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax)\n",
    "cbar.set_label('Log(Counts)')\n",
    "# Plot the identity line (the line where predicted equals truth)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.axline(xy1=(0, 0), slope=1, color='black')\n",
    "# Set axis limits and labels\n",
    "ax.set_xlim(lims[0], lims[1])\n",
    "ax.set_ylim(lims[0], lims[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_xlabel('Truth')\n",
    "ax.set_title('Effective Surface Area [unitless]')\n",
    "# Add R2 text annotation in relative figure coordinates\n",
    "ax.text(0.25, 0.75, f'$R^2$ = {r2:.2f}', transform=plt.gcf().transFigure, ha='left', size=16)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b95ae",
   "metadata": {},
   "source": [
    "# Vanilla CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bf5b4",
   "metadata": {},
   "source": [
    "## Regression: rho_eff and sa_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba37ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c89579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03567be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c131e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c113d021",
   "metadata": {},
   "source": [
    "## Classification: n_arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505934c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f0e6b6",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08724be8",
   "metadata": {},
   "source": [
    "## a) rho_eff and sa_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from models.mlp_regression import MLPRegression \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34daa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand = 666 # random seed\n",
    "data_dir = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/tabular-data-v2'\n",
    "data_file = 'ros-tabular-data.parquet'\n",
    "data_path = os.path.join(data_dir, data_file)\n",
    "\n",
    "# create train/test set\n",
    "df = pd.read_parquet(data_path)\n",
    "df = df[df['view']=='default']\n",
    "features = ['aspect_ratio', 'aspect_ratio_elip', 'extreme_pts', \n",
    "        'contour_area', 'contour_perimeter', 'area_ratio', 'complexity', \n",
    "        'circularity']\n",
    "targets = ['rho_eff', 'sa_eff']\n",
    "df = df.sample(70_000) # TEMP: get a subset for efficiency\n",
    "df.reset_index(inplace=True)\n",
    "df_features = df[features]\n",
    "df_targets = df[targets]\n",
    "X = df_features\n",
    "y = df_targets\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: 70% train, 30% temp (val + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=n_rand)\n",
    "# Second: 15% val, 15% test from the 30% temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=n_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ced4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5322360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert standardized data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "log_y_train_tensor = torch.log(y_train_tensor)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "log_y_val_tensor = torch.log(y_val_tensor)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "log_y_test_tensor = torch.log(y_test_tensor)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, log_y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, log_y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_test_tensor, log_y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = MLPRegression(input_size, output_size, learning_rate=learning_rate)\n",
    "\n",
    "# Set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'mlp-regression-subset-tb'\n",
    "csv_log_name = f'mlp-regression-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd197ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/mlp-regression-subset-csv/version_6/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 100  # Number of batches to process\n",
    "\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 10 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        # Apply inverse log transform to both predictions and targets\n",
    "        preds = torch.exp(preds)  # Inverse of log\n",
    "        targets = torch.exp(targets)  # Inverse of log\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Compute R² values\n",
    "r2_rho = r2_score(all_targets[:, 0], all_preds[:, 0])\n",
    "r2_sa = r2_score(all_targets[:, 1], all_preds[:, 1])\n",
    "print(f'R² for rho_eff: {r2_rho:.2f}')\n",
    "print(f'R² for sa_eff: {r2_sa:.2f}')\n",
    "\n",
    "# Plot scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for rho_eff\n",
    "axes[0].scatter(all_targets[:, 0], all_preds[:, 0], alpha=0.5, edgecolor='white')\n",
    "axes[0].plot([all_targets[:, 0].min(), all_targets[:, 0].max()],\n",
    "             [all_targets[:, 0].min(), all_targets[:, 0].max()], 'k--', lw=2)\n",
    "axes[0].set_title(f'rho_eff (R² = {r2_rho:.2f})')\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "\n",
    "# Scatter plot for sa_eff\n",
    "axes[1].scatter(all_targets[:, 1], all_preds[:, 1], alpha=0.5, edgecolor='white')\n",
    "axes[1].plot([all_targets[:, 1].min(), all_targets[:, 1].max()],\n",
    "             [all_targets[:, 1].min(), all_targets[:, 1].max()], 'k--', lw=2)\n",
    "axes[1].set_title(f'sa_eff (R² = {r2_sa:.2f})')\n",
    "axes[1].set_xlabel('True Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the MLP model\n",
    "# class SimpleMLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(SimpleMLP, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "# class ImprovedMLP(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(0.2)\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(0.2)\n",
    "#         self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Grid search parameters\n",
    "# num_epochs = 200\n",
    "# batch_size = 128\n",
    "# learning_rate = 0.001\n",
    "# hidden_size = 128\n",
    "# # patience = 10  # Early stopping patience\n",
    "\n",
    "# # Prepare data\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "# log_y_train_tensor = torch.log(y_train_tensor)\n",
    "\n",
    "# X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "# log_y_val_tensor = torch.log(y_val_tensor)\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# output_size = y_train.shape[1]\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "# # Apply log transform to y_train\n",
    "# # y_train_tensor = torch.log1p(y_train_tensor)\n",
    "# log_y_train_tensor = torch.log(y_train_tensor)\n",
    "\n",
    "# # Create DataLoader for batching\n",
    "# train_dataset = TensorDataset(X_train_tensor, log_y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Model, loss, and optimizer\n",
    "# input_size = X_train.shape[1]\n",
    "# output_size = y_train.shape[1]\n",
    "# model = ImprovedMLP(input_size, hidden_size, output_size)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164288b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid search \n",
    "# # Grid search parameters\n",
    "# hidden_sizes = [64, 128, 256]\n",
    "# batch_sizes = [32, 64, 128]\n",
    "# num_epochs = 100\n",
    "# patience = 10  # Early stopping patience\n",
    "\n",
    "# # Prepare data\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "# log_y_train_tensor = torch.log(y_train_tensor)\n",
    "\n",
    "# X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "# log_y_val_tensor = torch.log(y_val_tensor)\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# output_size = y_train.shape[1]\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# best_params = None\n",
    "# best_model_state = None\n",
    "\n",
    "# for hidden_size in hidden_sizes:\n",
    "#     for batch_size in batch_sizes:\n",
    "#         print(f\"\\nTraining with hidden_size={hidden_size}, batch_size={batch_size}\")\n",
    "#         model = ImprovedMLP(input_size, hidden_size, output_size)\n",
    "#         criterion = nn.MSELoss()\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#         train_dataset = TensorDataset(X_train_tensor, log_y_train_tensor)\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         val_dataset = TensorDataset(X_val_tensor, log_y_val_tensor)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         best_epoch_val_loss = float('inf')\n",
    "#         epochs_no_improve = 0\n",
    "\n",
    "#         for epoch in range(num_epochs):\n",
    "#             model.train()\n",
    "#             epoch_loss = 0.0\n",
    "#             for batch_X, batch_y in train_loader:\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(batch_X)\n",
    "#                 loss = criterion(outputs, batch_y)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 epoch_loss += loss.item()\n",
    "#             avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "#             # Validation\n",
    "#             model.eval()\n",
    "#             val_loss = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for val_X, val_y in val_loader:\n",
    "#                     val_outputs = model(val_X)\n",
    "#                     v_loss = criterion(val_outputs, val_y)\n",
    "#                     val_loss += v_loss.item()\n",
    "#             avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "#             if (epoch + 1) % 5 == 0:\n",
    "#                 print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#             # Early stopping\n",
    "#             if avg_val_loss < best_epoch_val_loss:\n",
    "#                 best_epoch_val_loss = avg_val_loss\n",
    "#                 epochs_no_improve = 0\n",
    "#                 best_epoch_model_state = model.state_dict()\n",
    "#             else:\n",
    "#                 epochs_no_improve += 1\n",
    "#                 if epochs_no_improve >= patience:\n",
    "#                     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                     break\n",
    "\n",
    "#         # Save best model for this config\n",
    "#         if best_epoch_val_loss < best_val_loss:\n",
    "#             best_val_loss = best_epoch_val_loss\n",
    "#             best_params = {'hidden_size': hidden_size, 'batch_size': batch_size}\n",
    "#             best_model_state = best_epoch_model_state\n",
    "\n",
    "# print(f\"\\nBest validation loss: {best_val_loss:.4f} with params: {best_params}\")\n",
    "\n",
    "# # To load the best model:\n",
    "# best_model = ImprovedMLP(input_size, best_params['hidden_size'], output_size)\n",
    "# best_model.load_state_dict(best_model_state)\n",
    "# best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_val to a PyTorch tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# # Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run model on validation data without computing gradients\n",
    "with torch.no_grad():\n",
    "    log_y_test_pred = model(X_test_tensor).numpy()\n",
    "    y_test_pred = np.exp(log_y_test_pred)  # Inverse log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078226cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dataframe\n",
    "df_test_out = y_test.rename(columns={'rho_eff':'rho_eff_truth', 'sa_eff':'sa_eff_truth'})\n",
    "df_test_out['n_arms'] = df['n_arms'].iloc[X_test.index]\n",
    "df_test_out['filename'] = df['filename'].iloc[X_test.index]\n",
    "df_test_out['rho_eff_pred'] = y_test_pred[:, 0]\n",
    "df_test_out['sa_eff_pred'] = y_test_pred[:, 1]\n",
    "\n",
    "# get score\n",
    "r2_rho = r2_score(df_test_out['rho_eff_truth'], df_test_out['rho_eff_pred'])\n",
    "r2_sa = r2_score(df_test_out['sa_eff_truth'], df_test_out['sa_eff_pred'])\n",
    "print(f'R2 for effective density = {r2_rho}')\n",
    "print(f'R2 for effective surface area = {r2_sa}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90446ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# df_subset = df_val_out.sample(100_000, random_state=n_rand)\n",
    "df_subset = df_test_out\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.scatterplot(data=df_subset, x='rho_eff_pred', y='rho_eff_truth', \n",
    "hue='n_arms', alpha=0.5,\n",
    "legend='full', edgecolor='white')\n",
    "# sns.scatterplot(data=df_subset, x='rho_pred', y='rho_truth', \n",
    "# size='n_arms', alpha=0.5, palette=mpl.colormaps['plasma'], \n",
    "# legend='full')\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "# now plot both limits against eachother\n",
    "# ax.plot(lims, lims, 'k-', alpha=0.7, zorder=0)\n",
    "ax.axline(xy1=(0,0), slope=1, color='black')\n",
    "ax.set_xlim(lims[0], lims[1])\n",
    "ax.set_ylim(lims[0], lims[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_ylabel('true')\n",
    "ax.set_xlabel('predicted')\n",
    "ax.set_title('Effective density [unitless]')\n",
    "ax.text(0.2, 0.75, f'$R^2$ = {r2_rho:.2f}', transform=plt.gcf().transFigure, ha='left', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# df_subset = df_val_out.sample(100_000, random_state=n_rand)\n",
    "df_subset = df_test_out\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.scatterplot(data=df_subset, x='sa_eff_pred', y='sa_eff_truth', \n",
    "hue='n_arms', alpha=0.5,\n",
    "legend='full', edgecolor='white')\n",
    "# sns.scatterplot(data=df_subset, x='rho_pred', y='rho_truth', \n",
    "# size='n_arms', alpha=0.5, palette=mpl.colormaps['plasma'], \n",
    "# legend='full')\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "# now plot both limits against eachother\n",
    "# ax.plot(lims, lims, 'k-', alpha=0.7, zorder=0)\n",
    "ax.axline(xy1=(0,0), slope=1, color='black')\n",
    "ax.set_xlim(lims[0], lims[1])\n",
    "ax.set_ylim(lims[0], lims[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_ylabel('true')\n",
    "ax.set_xlabel('predicted')\n",
    "ax.set_title('Effective surface area [unitless]')\n",
    "ax.text(0.2, 0.75, f'$R^2$ = {r2_sa:.2f}', transform=plt.gcf().transFigure, ha='left', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982233f8",
   "metadata": {},
   "source": [
    "## b) n_arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d157940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from models.mlp_classification import MLPClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand = 666 # random seed\n",
    "data_dir = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/tabular-data-v2'\n",
    "data_file = 'ros-tabular-data.parquet'\n",
    "data_path = os.path.join(data_dir, data_file)\n",
    "\n",
    "# create train/test set\n",
    "df = pd.read_parquet(data_path)\n",
    "df = df[df['view']=='default']\n",
    "features = ['aspect_ratio', 'aspect_ratio_elip', 'extreme_pts', \n",
    "        'contour_area', 'contour_perimeter', 'area_ratio', 'complexity', \n",
    "        'circularity']\n",
    "targets = ['n_arms']\n",
    "df = df.sample(70_000) # TEMP: get a subset for efficiency\n",
    "df.reset_index(inplace=True)\n",
    "df_features = df[features]\n",
    "df_targets = df[targets]\n",
    "X = df_features\n",
    "y = df_targets\n",
    "print(len(X), len(y))\n",
    "# First: 70% train, 30% temp (val + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=n_rand)\n",
    "# Second: 15% val, 15% test from the 30% temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=n_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters \n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Load the class mapping from a JSON file\n",
    "class_mapping_file = '/glade/u/home/joko/ice3d/data/class_to_idx.json'\n",
    "# Load class mapping from JSON file\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "num_classes = len(class_to_idx)  # Number of unique classes in n_arms\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert standardized data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor([class_to_idx[str(v)] for v in y_train['n_arms'].values], dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor([class_to_idx[str(v)] for v in y_val['n_arms'].values], dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor([class_to_idx[str(v)] for v in y_test['n_arms'].values], dtype=torch.long)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = MLPClassification(input_size, num_classes, learning_rate=learning_rate)\n",
    "\n",
    "# Set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'mlp-classification-subset-tb'\n",
    "csv_log_name = f'mlp-classification-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ccfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/mlp-classification-subset-csv/version_2/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "# Run inference on the test data\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        preds = torch.argmax(preds, dim=1)  # Get the predicted class\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Map predictions and targets back to original labels\n",
    "idx_to_class = {v: int(float(k)) for k, v in class_to_idx.items()}  # Reverse the class mapping robustly\n",
    "all_preds_mapped = [idx_to_class[p] for p in all_preds]\n",
    "all_targets_mapped = [idx_to_class[t] for t in all_targets]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_targets_mapped, all_preds_mapped, normalize='true')\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_labels = list(idx_to_class.values())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d690071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model for classification\n",
    "class SimpleMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleMLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Update the ImprovedMLP class for classification\n",
    "class ImprovedMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# set hyperparameters \n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "hidden_size = 128\n",
    "\n",
    "# Prepare data for classification\n",
    "num_classes = len(y_train['n_arms'].unique())  # Number of unique classes in n_arms\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# Map n_arms labels from [4,5,6,7,8,9,10] to [0,1,2,3,4,5,6]\n",
    "label_map = {v: i for i, v in enumerate(sorted(y_train['n_arms'].unique()))}\n",
    "y_train_tensor = torch.tensor([label_map[v] for v in y_train['n_arms'].values], dtype=torch.long)\n",
    "y_val_tensor = torch.tensor([label_map[v] for v in y_val['n_arms'].values], dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = ImprovedMLPClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # Classification loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4487f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "# Set model to evaluation mode\n",
    "model.eval()    \n",
    "# Run model on validation data without computing gradients\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor)\n",
    "    _, preds = torch.max(y_test_pred, 1)\n",
    "\n",
    "# After prediction\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "pred_labels = [inv_label_map[int(i)] for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Extract predictions and targets\n",
    "preds = pred_labels\n",
    "targets = y_test.values.ravel()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(targets, preds, normalize='true')\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_labels = ['4', '5', '6', '7', '8', '9', '10']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe57e06",
   "metadata": {},
   "source": [
    "# Test python module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cdfea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.single_view_dataset import SingleViewDataset\n",
    "from data.single_view_datamodule import SingleViewDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab662b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your HDF5 file and the target dataset name\n",
    "hdf_path = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "target_name = 'n_arms'  # or 'sa_eff', 'n_arms', etc.\n",
    "indices = list(range(1_000))\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = SingleViewDataset(hdf_path, target_name, indices)\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "# Get a random sample\n",
    "random_idx = random.randint(0, len(dataset) - 1)\n",
    "img_tensor, target = dataset[random_idx]\n",
    "print(f\"Image tensor shape: {img_tensor.shape}, dtype: {img_tensor.dtype}\")\n",
    "print(f\"Target: {target}, dtype: {target.dtype}\")\n",
    "\n",
    "# Visualize the image (convert from (3, H, W) to (H, W, 3))\n",
    "img_np = img_tensor.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "plt.imshow(img_np)\n",
    "plt.title(f\"Target: {target.item():.3f}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set your parameters\n",
    "hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "batch_size = 64\n",
    "target = 'n_arms'  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = np.arange(7_000_000)\n",
    "# indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "# sort all idx numpy arrays\n",
    "train_idx.sort()\n",
    "val_idx.sort()\n",
    "test_idx.sort()\n",
    "subset_size = 0.001  # subset percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Set your parameters\n",
    "# hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "# batch_size = 64\n",
    "# target = 'n_arms'  # Replace with your actual target column name\n",
    "# random_state = 666\n",
    "# # specify train/val/test indices using sklearn\n",
    "# indices = np.arange(7_000_000)\n",
    "# train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "# val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "# subset_size = 0.001  # subset percentage\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('checkpoint 1')\n",
    "dm = SingleViewDataModule(hdf_file, target, train_idx, val_idx, test_idx, batch_size, subset_size=0.01, subset_seed=random_state)\n",
    "\n",
    "# Setup the datasets\n",
    "print('checkpoint 2')\n",
    "dm.setup()\n",
    "\n",
    "# Get a batch from the train dataloader\n",
    "print('checkpoint 3')\n",
    "train_loader = dm.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, (tuple, list)):\n",
    "    print(f\"Batch[0] shape: {batch[0].shape}\")\n",
    "else:\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "\n",
    "# Visualize all the images in the batch as a grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if isinstance(batch, (tuple, list)):\n",
    "    images = batch[0]\n",
    "else:\n",
    "    images = batch\n",
    "\n",
    "if isinstance(images, torch.Tensor) and images.ndim == 4:  # Ensure it's a batch of images\n",
    "    batch_size = images.shape[0]\n",
    "    grid_size = int(np.ceil(np.sqrt(batch_size)))  # Determine grid size\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        img = images[i]\n",
    "        if img.ndim == 3:  # Ensure it's a single image tensor\n",
    "            axes[i].imshow(img.permute(1, 2, 0).numpy().astype('uint8'))\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(batch_size, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the length of train/val/test datasets?\n",
    "print(f\"Train dataset length: {len(dm.train_dataset)}\")\n",
    "print(f\"Validation dataset length: {len(dm.val_dataset)}\")\n",
    "print(f\"Test dataset length: {len(dm.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, (tuple, list)):\n",
    "    print(f\"Batch[0] shape: {batch[0].shape}\")\n",
    "else:\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "\n",
    "# Visualize all the images in the batch as a grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if isinstance(batch, (tuple, list)):\n",
    "    images = batch[0]\n",
    "else:\n",
    "    images = batch\n",
    "\n",
    "if isinstance(images, torch.Tensor) and images.ndim == 4:  # Ensure it's a batch of images\n",
    "    batch_size = images.shape[0]\n",
    "    grid_size = int(np.ceil(np.sqrt(batch_size)))  # Determine grid size\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        img = images[i]\n",
    "        if img.ndim == 3:  # Ensure it's a single image tensor\n",
    "            axes[i].imshow(img.permute(1, 2, 0).numpy().astype('uint8'))\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(batch_size, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9ca2a",
   "metadata": {},
   "source": [
    "# Training with h5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51c7c8",
   "metadata": {},
   "source": [
    "## Single view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821998a",
   "metadata": {},
   "source": [
    "### CNN (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.single_view_dataset import SingleViewDataset\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from models.cnn_regression import VanillaCNNRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set up dataset and dataloader\n",
    "\n",
    "# Set your parameters\n",
    "hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "batch_size = 128\n",
    "targets = ['rho_eff', 'sa_eff']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 30\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 0.05  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'cnn-vanilla-regression-subset-tb'\n",
    "csv_log_name = f'cnn-vanilla-regression-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0, 1.0])\n",
    "])\n",
    "\n",
    "# Output transform: log-transform\n",
    "def log_transform(target):\n",
    "    # return torch.log1p(target)  # log(1 + x) for numerical stability\n",
    "    return torch.log(target)  # log(x) for positive values\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=18,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")\n",
    "\n",
    "# train VanillaCNNRegression\n",
    "print('set up model and trainer...')\n",
    "model = VanillaCNNRegression(input_channels=3, output_size=len(targets), learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                  strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/cnn-vanilla-regression-subset-csv/version_16/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=18,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of samples in train, validation, and test datasets\n",
    "print(f\"Number of samples in train dataset: {len(dm.train_dataset)}\")\n",
    "print(f\"Number of samples in validation dataset: {len(dm.val_dataset)}\")\n",
    "print(f\"Number of samples in test dataset: {len(dm.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa803b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "test_loader = dm.test_dataloader()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 20  # Number of batches to process\n",
    "\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 20 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        # Apply inverse log transform to both predictions and targets\n",
    "        preds = torch.exp(preds)  # Inverse of log\n",
    "        targets = torch.exp(targets)  # Inverse of log\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Compute R² values\n",
    "r2_rho = r2_score(all_targets[:, 0], all_preds[:, 0])\n",
    "r2_sa = r2_score(all_targets[:, 1], all_preds[:, 1])\n",
    "print(f'R² for rho_eff: {r2_rho:.2f}')\n",
    "print(f'R² for sa_eff: {r2_sa:.2f}')\n",
    "\n",
    "# Plot scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for rho_eff\n",
    "axes[0].scatter(all_targets[:, 0], all_preds[:, 0], alpha=0.5, edgecolor='white')\n",
    "axes[0].plot([all_targets[:, 0].min(), all_targets[:, 0].max()],\n",
    "             [all_targets[:, 0].min(), all_targets[:, 0].max()], 'k--', lw=2)\n",
    "axes[0].set_title(f'rho_eff (R² = {r2_rho:.2f})')\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "\n",
    "# Scatter plot for sa_eff\n",
    "axes[1].scatter(all_targets[:, 1], all_preds[:, 1], alpha=0.5, edgecolor='white')\n",
    "axes[1].plot([all_targets[:, 1].min(), all_targets[:, 1].max()],\n",
    "             [all_targets[:, 1].min(), all_targets[:, 1].max()], 'k--', lw=2)\n",
    "axes[1].set_title(f'sa_eff (R² = {r2_sa:.2f})')\n",
    "axes[1].set_xlabel('True Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b9d76",
   "metadata": {},
   "source": [
    "### Resnet (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.single_view_dataset import SingleViewDataset\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from models.resnet18_regression import ResNet18Regression\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set up dataset and dataloader\n",
    "\n",
    "# Set your parameters\n",
    "hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "batch_size = 128\n",
    "targets = ['rho_eff', 'sa_eff']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 10\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "num_cpus = 18\n",
    "\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 0.01  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'resnet18-regression-subset-tb'\n",
    "csv_log_name = f'resnet18-regression-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5],       # Zero-mean normalization\n",
    "                         std=[1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5],       # Zero-mean normalization\n",
    "                         std=[1.0])\n",
    "])\n",
    "\n",
    "# Output transform: log-transform\n",
    "def log_transform(target):\n",
    "    # return torch.log1p(target)  # log(1 + x) for numerical stability\n",
    "    return torch.log(target)  # log(x) for positive values\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=16,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")\n",
    "\n",
    "# train model\n",
    "print('set up model and trainer...')\n",
    "model = ResNet18Regression(input_channels=1, output_size=len(targets), learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                  strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecebffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/resnet18-regression-subset-csv/version_8/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b30ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "# test_loader = dm.test_dataloader()\n",
    "test_loader = torch.utils.data.DataLoader(dm.test_dataset, batch_size=128, shuffle=True)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 10  # Number of batches to process\n",
    "\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 20 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        # Apply inverse log transform to both predictions and targets\n",
    "        preds = torch.exp(preds)  # Inverse of log\n",
    "        targets = torch.exp(targets)  # Inverse of log\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Compute R² values\n",
    "r2_rho = r2_score(all_targets[:, 0], all_preds[:, 0])\n",
    "r2_sa = r2_score(all_targets[:, 1], all_preds[:, 1])\n",
    "print(f'R² for rho_eff: {r2_rho:.2f}')\n",
    "print(f'R² for sa_eff: {r2_sa:.2f}')\n",
    "\n",
    "# Plot scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for rho_eff\n",
    "axes[0].scatter(all_targets[:, 0], all_preds[:, 0], alpha=0.5, edgecolor='white')\n",
    "axes[0].plot([all_targets[:, 0].min(), all_targets[:, 0].max()],\n",
    "             [all_targets[:, 0].min(), all_targets[:, 0].max()], 'k--', lw=2)\n",
    "axes[0].set_title(f'rho_eff (R² = {r2_rho:.2f})')\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "\n",
    "# Scatter plot for sa_eff\n",
    "axes[1].scatter(all_targets[:, 1], all_preds[:, 1], alpha=0.5, edgecolor='white')\n",
    "axes[1].plot([all_targets[:, 1].min(), all_targets[:, 1].max()],\n",
    "             [all_targets[:, 1].min(), all_targets[:, 1].max()], 'k--', lw=2)\n",
    "axes[1].set_title(f'sa_eff (R² = {r2_sa:.2f})')\n",
    "axes[1].set_xlabel('True Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68987a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2e2d5d",
   "metadata": {},
   "source": [
    "### CNN (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.single_view_dataset import SingleViewDataset\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from models.cnn_classification import VanillaCNNClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0250b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING\n",
    "# Set your parameters\n",
    "hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/shuffled_small/default_shuffled_small.h5'\n",
    "batch_size = 128\n",
    "targets = ['n_arms']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 10\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "num_cpus = 18\n",
    "\n",
    "# # specify train/val/test indices using sklearn\n",
    "# indices = list(range(7_000_000))\n",
    "# train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "# val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 1.0  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'cnn-vanilla-classification-subset-tb'\n",
    "csv_log_name = f'cnn-vanilla-classification-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5],       # Zero-mean normalization\n",
    "                         std=[1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5],       # Zero-mean normalization\n",
    "                         std=[1.0])\n",
    "])\n",
    "\n",
    "class_mapping_file = '/glade/u/home/joko/ice3d/data/class_to_idx.json'\n",
    "# Load class mapping from JSON file\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "num_classes = len(class_to_idx)  # Number of unique classes in n_arms\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=None,\n",
    "    val_idx=None,\n",
    "    test_idx=None,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=None,\n",
    "    val_target_transform=None,\n",
    "    test_target_transform=None,\n",
    "    task_type='classification',  # Specify the task type\n",
    "    class_to_idx=class_to_idx  # Optional: mapping of class names to indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()\n",
    "# print a subset of training labels to QA\n",
    "print(\"Sample training labels:\")\n",
    "for i in range(10):\n",
    "    print(dm.train_dataset[i][1])  # Assuming the target is the second element in the tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VanillaCNNClassification\n",
    "print('set up model and trainer...')\n",
    "model = VanillaCNNClassification(input_channels=1, num_classes=num_classes, learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                #   strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set up dataset and dataloader\n",
    "\n",
    "# Set your parameters\n",
    "hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "batch_size = 128\n",
    "targets = ['n_arms']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 10\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "num_cpus = 18\n",
    "\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 0.01  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'cnn-vanilla-classification-subset-tb'\n",
    "csv_log_name = f'cnn-vanilla-classification-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0, 1.0])\n",
    "])\n",
    "\n",
    "class_mapping_file = '/glade/u/home/joko/ice3d/data/class_to_idx.json'\n",
    "# Load class mapping from JSON file\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "num_classes = len(class_to_idx)  # Number of unique classes in n_arms\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=None,\n",
    "    val_target_transform=None,\n",
    "    test_target_transform=None,\n",
    "    task_type='classification',  # Specify the task type\n",
    "    class_to_idx=class_to_idx  # Optional: mapping of class names to indices\n",
    ")\n",
    "\n",
    "# train VanillaCNNClassification\n",
    "print('set up model and trainer...')\n",
    "model = VanillaCNNClassification(input_channels=3, num_classes=num_classes, learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                  strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/cnn-vanilla-classification-subset-csv/version_4/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)         \n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dm again\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=18,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=None,\n",
    "    val_target_transform=None,\n",
    "    test_target_transform=None,\n",
    "    task_type='classification',  # Specify the task type\n",
    "    class_to_idx=class_to_idx  # Optional: mapping of class names to indices\n",
    ")\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print unique classes in train, validation, and test datasets\n",
    "# import numpy as np\n",
    "\n",
    "# def get_unique_targets(dataset):\n",
    "#     targets = []\n",
    "#     for i in range(len(dataset)):\n",
    "#         _, target = dataset[i]\n",
    "#         targets.append(int(target) if hasattr(target, 'item') else target)\n",
    "#     return np.unique(targets)\n",
    "\n",
    "# print(f\"Unique classes in train dataset: {get_unique_targets(dm.train_dataset)}\")\n",
    "# print(f\"Unique classes in validation dataset: {get_unique_targets(dm.val_dataset)}\")\n",
    "# print(f\"Unique classes in test dataset: {get_unique_targets(dm.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to evaluation mode\n",
    "model.eval()\n",
    "# run inference on test set\n",
    "# test_loader = dm.test_dataloader()\n",
    "test_loader = torch.utils.data.DataLoader(dm.test_dataset, batch_size=128, shuffle=True)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 20  # Number of batches to process\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 20 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Convert model outputs to predicted class indices\n",
    "pred_labels = np.argmax(all_preds, axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(all_targets, pred_labels)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "cm = confusion_matrix(all_targets, pred_labels, normalize='true')\n",
    "class_labels = list(class_to_idx.keys())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a93bd",
   "metadata": {},
   "source": [
    "### Resnet (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.single_view_dataset import SingleViewDataset\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from models.resnet18_classification import ResNet18Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set up dataset and dataloader\n",
    "\n",
    "# Set your parameters\n",
    "hdf_file = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "batch_size = 128\n",
    "targets = ['n_arms']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 30\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "num_cpus = 18\n",
    "\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 0.05  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'resnet18-classification-subset-tb'\n",
    "csv_log_name = f'resnet18-classification-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0, 1.0])\n",
    "])\n",
    "\n",
    "class_mapping_file = '/glade/u/home/joko/ice3d/data/class_to_idx.json'\n",
    "# Load class mapping from JSON file\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "num_classes = len(class_to_idx)  # Number of unique classes in n_arms\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=None,\n",
    "    val_target_transform=None,\n",
    "    test_target_transform=None,\n",
    "    task_type='classification',  # Specify the task type\n",
    "    class_to_idx=class_to_idx  # Optional: mapping of class names to indices\n",
    ")\n",
    "\n",
    "# train VanillaCNNClassification\n",
    "print('set up model and trainer...')\n",
    "model = ResNet18Classification(input_channels=3, num_classes=num_classes, learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                  strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/resnet18-classification-subset-csv/version_2/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)         \n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dm again\n",
    "dm = SingleViewDataModule(\n",
    "    hdf_file=hdf_file,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=18,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=None,\n",
    "    val_target_transform=None,\n",
    "    test_target_transform=None,\n",
    "    task_type='classification',  # Specify the task type\n",
    "    class_to_idx=class_to_idx  # Optional: mapping of class names to indices\n",
    ")\n",
    "dm.setup()\n",
    "\n",
    "# set model to evaluation mode\n",
    "model.eval()\n",
    "# run inference on test set\n",
    "# test_loader = dm.test_dataloader()\n",
    "test_loader = torch.utils.data.DataLoader(dm.test_dataset, batch_size=128, shuffle=True)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 20  # Number of batches to process\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 20 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Convert model outputs to predicted class indices\n",
    "pred_labels = np.argmax(all_preds, axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(all_targets, pred_labels)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "cm = confusion_matrix(all_targets, pred_labels, normalize='true')\n",
    "class_labels = list(class_to_idx.keys())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bb6a0",
   "metadata": {},
   "source": [
    "## Stereo View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0780852",
   "metadata": {},
   "source": [
    "### CNN (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e47a2",
   "metadata": {},
   "source": [
    "#### 2DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jko/ice3d')\n",
    "from data.stereo_view_dataset import StereoViewDataset\n",
    "from data.stereo_view_datamodule import StereoViewDataModule\n",
    "from models.cnn_regression import VanillaCNNRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiate datamodule...\n"
     ]
    }
   ],
   "source": [
    "# Set up dataset and dataloader\n",
    "# Set your parameters\n",
    "hdf_file_1 = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small/default_shuffled_small.h5'\n",
    "hdf_file_2 = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small/2ds_shuffled_small.h5'\n",
    "batch_size = 128\n",
    "targets = ['rho_eff', 'sa_eff']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 2\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "num_cpus = 16\n",
    "subset_size = 1.0  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/home/jko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'stereo-2ds-cnn-regression-subset-tb'\n",
    "csv_log_name = f'stereo-2ds-cnn-regression-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0])\n",
    "])\n",
    "\n",
    "# Output transform: log-transform\n",
    "def log_transform(target):\n",
    "    # return torch.log1p(target)  # log(1 + x) for numerical stability\n",
    "    return torch.log(target)  # log(x) for positive values\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = StereoViewDataModule(\n",
    "    hdf_file_left=hdf_file_1,\n",
    "    hdf_file_right=hdf_file_2,\n",
    "    target_names=targets,\n",
    "    train_idx=None,\n",
    "    val_idx=None,\n",
    "    test_idx=None,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=8,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up model and trainer...\n",
      "Training model with 1 gpu(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | conv1 | Conv2d    | 304    | train\n",
      "1 | conv2 | Conv2d    | 4.6 K  | train\n",
      "2 | conv3 | Conv2d    | 18.5 K | train\n",
      "3 | pool  | MaxPool2d | 0      | train\n",
      "4 | fc1   | Linear    | 6.4 M  | train\n",
      "5 | fc2   | Linear    | 8.3 K  | train\n",
      "6 | fc3   | Linear    | 130    | train\n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.818    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3828/3828 [01:49<00:00, 35.02it/s, v_num=9]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3828/3828 [01:49<00:00, 34.91it/s, v_num=9]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "print('set up model and trainer...')\n",
    "model = VanillaCNNRegression(input_channels=2, output_size=len(targets), learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"gpu\",\n",
    "                  devices=1,\n",
    "                #   strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f96b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4512b7a",
   "metadata": {},
   "source": [
    "#### PHIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa019ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a680f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7c293c",
   "metadata": {},
   "source": [
    "### Resnet (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa76bc",
   "metadata": {},
   "source": [
    "#### 2DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5750f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m/glade/u/home/joko/ice3d\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstereo_view_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StereoViewDataset\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstereo_view_datamodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StereoViewDataModule\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresnet18_regression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet18Regression\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.stereo_view_dataset import StereoViewDataset\n",
    "from data.stereo_view_datamodule import StereoViewDataModule\n",
    "from models.resnet18_regression import ResNet18Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instantiate datamodule...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StereoViewDataModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:43\u001b[39m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'StereoViewDataModule' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set up dataset and dataloader\n",
    "# Set your parameters\n",
    "hdf_file_1 = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small/default_shuffled_small.h5'\n",
    "hdf_file_2 = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small/2ds_shuffled_small.h5'\n",
    "batch_size = 128\n",
    "targets = ['rho_eff', 'sa_eff']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 2\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 3\n",
    "num_cpus = 32\n",
    "subset_size = 1.0  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'stereo-2ds-resnet18-regression-subset-tb'\n",
    "csv_log_name = f'stereo-2ds-resnet18-regression-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0])\n",
    "])\n",
    "\n",
    "# Output transform: log-transform\n",
    "def log_transform(target):\n",
    "    # return torch.log1p(target)  # log(1 + x) for numerical stability\n",
    "    return torch.log(target)  # log(x) for positive values\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = StereoViewDataModule(\n",
    "    hdf_file_left=hdf_file_1,\n",
    "    hdf_file_right=hdf_file_2,\n",
    "    target_names=targets,\n",
    "    train_idx=None,\n",
    "    val_idx=None,\n",
    "    test_idx=None,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=32,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")\n",
    "\n",
    "# train model\n",
    "print('set up model and trainer...')\n",
    "model = ResNet18Regression(input_channels=2, output_size=len(targets), learning_rate=lr)\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                  strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a4b4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-regression-subset-csv/version_5/metrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n",
      "\u001b[32m      2\u001b[39m log_path = \u001b[33m'\u001b[39m\u001b[33m/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-regression-subset-csv/version_5/metrics.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Read the metrics.csv file using pandas\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m metrics_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Inspect the columns of the DataFrame (to ensure it's structured properly)\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(metrics_df.columns)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n",
      "\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n",
      "\u001b[32m   1014\u001b[39m     dialect,\n",
      "\u001b[32m   1015\u001b[39m     delimiter,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n",
      "\u001b[32m   1023\u001b[39m )\n",
      "\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n",
      "\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n",
      "\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n",
      "\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n",
      "\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n",
      "\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n",
      "\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n",
      "\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-regression-subset-csv/version_5/metrics.csv'"
     ]
    }
   ],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-regression-subset-csv/version_5/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9dc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters\n",
    "hdf_file_1 = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/default.h5'\n",
    "hdf_file_2 = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/2ds.h5'\n",
    "batch_size = 64\n",
    "targets = ['rho_eff', 'sa_eff']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 5e-4\n",
    "n_epochs = 40\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 1\n",
    "num_cpus = 18\n",
    "\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 0.05  # subset percentage\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0])\n",
    "])\n",
    "\n",
    "# Output transform: log-transform\n",
    "def log_transform(target):\n",
    "    # return torch.log1p(target)  # log(1 + x) for numerical stability\n",
    "    return torch.log(target)  # log(x) for positive values\n",
    "\n",
    "dm = StereoViewDataModule(\n",
    "    hdf_file_left=hdf_file_1,\n",
    "    hdf_file_right=hdf_file_2,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=4,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=log_transform,\n",
    "    val_target_transform=log_transform,\n",
    "    test_target_transform=log_transform\n",
    ")\n",
    "dm.setup()\n",
    "\n",
    "# load model from lightning_logs\n",
    "model_path = '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-regression-subset-csv/version_5/checkpoints/epoch=39-step=153120.ckpt'\n",
    "model = ResNet18Regression.load_from_checkpoint(\n",
    "    model_path,\n",
    "    input_channels=2,\n",
    "    output_size=len(targets),\n",
    "    learning_rate=lr\n",
    ")\n",
    "model = model.to('cpu')\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "test_loader = torch.utils.data.DataLoader(dm.test_dataset, batch_size=128, shuffle=True)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 20  # Number of batches to process\n",
    "\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 20 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        preds = model(images)\n",
    "        # Apply inverse log transform to both predictions and targets\n",
    "        preds = torch.exp(preds)  # Inverse of log\n",
    "        targets = torch.exp(targets)  # Inverse of log\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Compute R² values\n",
    "r2_rho = r2_score(all_targets[:, 0], all_preds[:, 0])\n",
    "r2_sa = r2_score(all_targets[:, 1], all_preds[:, 1])\n",
    "print(f'R² for rho_eff: {r2_rho:.2f}')\n",
    "print(f'R² for sa_eff: {r2_sa:.2f}')\n",
    "\n",
    "# Plot scatter plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Scatter plot for rho_eff\n",
    "axes[0].scatter(all_targets[:, 0], all_preds[:, 0], alpha=0.5, edgecolor='white')\n",
    "axes[0].plot([all_targets[:, 0].min(), all_targets[:, 0].max()],\n",
    "             [all_targets[:, 0].min(), all_targets[:, 0].max()], 'k--', lw=2)\n",
    "axes[0].set_title(f'rho_eff (R² = {r2_rho:.2f})')\n",
    "axes[0].set_xlabel('True Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "\n",
    "# Scatter plot for sa_eff\n",
    "axes[1].scatter(all_targets[:, 1], all_preds[:, 1], alpha=0.5, edgecolor='white')\n",
    "axes[1].plot([all_targets[:, 1].min(), all_targets[:, 1].max()],\n",
    "             [all_targets[:, 1].min(), all_targets[:, 1].max()], 'k--', lw=2)\n",
    "axes[1].set_title(f'sa_eff (R² = {r2_sa:.2f})')\n",
    "axes[1].set_xlabel('True Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c78ff9",
   "metadata": {},
   "source": [
    "#### PHIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bb772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb233f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a76b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf1d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "501e2c7a",
   "metadata": {},
   "source": [
    "### Resnet (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935334b",
   "metadata": {},
   "source": [
    "#### 2DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/joko/ice3d')\n",
    "from data.stereo_view_dataset import StereoViewDataset\n",
    "from data.stereo_view_datamodule import StereoViewDataModule\n",
    "from models.resnet18_classification import ResNet18Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset and dataloader\n",
    "# Set your parameters\n",
    "hdf_file_1 = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/sorted/default_sorted.h5'\n",
    "hdf_file_2 = '/glade/derecho/scratch/joko/synth-ros/params_200_50_20250403/imgs-ml-ready/sorted/2ds_sorted.h5'\n",
    "batch_size = 128\n",
    "targets = ['n_arms']  # Replace with your actual target column name\n",
    "random_state = 666\n",
    "lr = 1e-3\n",
    "n_epochs = 20\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "num_gpus = 4\n",
    "num_cpus = 64\n",
    "\n",
    "# specify train/val/test indices using sklearn\n",
    "indices = list(range(7_000_000))\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=random_state)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=random_state)\n",
    "subset_size = 0.10  # subset percentage\n",
    "\n",
    "# set up logger information\n",
    "log_dir = '/glade/u/home/joko/ice3d/models/lightning_logs'\n",
    "tb_log_name = f'stereo-resnet18-classification-subset-tb'\n",
    "csv_log_name = f'stereo-resnet18-classification-subset-csv'\n",
    "tb_logger = TensorBoardLogger(log_dir, name=tb_log_name)\n",
    "csv_logger = CSVLogger(log_dir, name=csv_log_name)\n",
    "\n",
    "# Input transform: random crop, flip, normalization, etc.\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0]),\n",
    "])\n",
    "\n",
    "# validation transform: resize, normalization\n",
    "val_transform = T.Compose([\n",
    "    transforms.Normalize(mean=[0.5, 0.5],       # Zero-mean normalization\n",
    "                         std=[1.0, 1.0])\n",
    "])\n",
    "\n",
    "class_mapping_file = '/glade/u/home/joko/ice3d/data/class_to_idx.json'\n",
    "# Load class mapping from JSON file\n",
    "with open(class_mapping_file, 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "num_classes = len(class_to_idx)  # Number of unique classes in n_arms\n",
    "\n",
    "# Instantiate the DataModule\n",
    "print('instantiate datamodule...')\n",
    "dm = StereoViewDataModule(\n",
    "    hdf_file_left=hdf_file_1,\n",
    "    hdf_file_right=hdf_file_2,\n",
    "    target_names=targets,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=subset_size,\n",
    "    subset_seed=random_state,\n",
    "    num_workers=num_cpus,\n",
    "    prefetch_factor=32,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    test_transform=val_transform,  # Often same as val\n",
    "    train_target_transform=None,\n",
    "    val_target_transform=None,\n",
    "    test_target_transform=None,\n",
    "    task_type='classification',  # Specify the task type\n",
    "    class_to_idx=class_to_idx  # Optional: mapping of class names to indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model\n",
    "print('set up model and trainer...')\n",
    "model = ResNet18Classification(input_channels=2, num_classes=num_classes, learning_rate=lr)\n",
    "\n",
    "# # load model from lightning_logs\n",
    "# model_path = '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-classification-subset-csv/version_16/checkpoints/epoch=12-step=42108.ckpt'\n",
    "# model = ResNet18Classification.load_from_checkpoint(\n",
    "#     model_path,\n",
    "#     input_channels=2,\n",
    "#     num_classes=num_classes,\n",
    "#     learning_rate=lr,\n",
    "#     map_location='cpu'  # Load model to CPU\n",
    "# )\n",
    "\n",
    "trainer = Trainer(max_epochs=n_epochs, \n",
    "                  accelerator=\"auto\",\n",
    "                  devices=num_gpus,\n",
    "                  strategy=\"ddp_notebook\", # distributed data parallel\n",
    "                  logger=[csv_logger, tb_logger],\n",
    "                  enable_progress_bar=True)\n",
    "\n",
    "# train model\n",
    "print(f'Training model with {num_gpus} gpu(s)...')\n",
    "trainer.fit(model, dm)\n",
    "# trainer.fit(model, dm, ckpt_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "log_path = '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-classification-subset-csv/version_17/metrics.csv'\n",
    "# Read the metrics.csv file using pandas\n",
    "metrics_df = pd.read_csv(log_path)\n",
    "# Inspect the columns of the DataFrame (to ensure it's structured properly)\n",
    "print(metrics_df.columns)\n",
    "# Group by 'epoch' and aggregate using the mean (or use 'last' for the final step of each epoch)\n",
    "metrics_df = metrics_df.groupby('epoch').agg({\n",
    "    'train_loss': 'mean',   # Take the mean of the training loss over steps in the same epoch\n",
    "    'val_loss': 'mean',     # Take the mean of the validation loss over steps in the same epoch\n",
    "}).reset_index()\n",
    "# Plot the loss curve (for training and validation losses)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# You can plot the training and validation loss curves if both are available in the CSV\n",
    "if 'train_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['train_loss'], label='Train Loss')\n",
    "if 'val_loss' in metrics_df.columns:\n",
    "    plt.plot(metrics_df['epoch'], metrics_df['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference on test set\n",
    "# NOTE: must re-instantiate dm to load the test dataset (cell above)\n",
    "dm.setup()\n",
    "\n",
    "# load model from lightning_logs\n",
    "model_path = '/glade/u/home/joko/ice3d/models/lightning_logs/stereo-resnet18-classification-subset-csv/version_17/checkpoints/epoch=19-step=19140.ckpt'\n",
    "model = ResNet18Classification.load_from_checkpoint(\n",
    "    model_path,\n",
    "    input_channels=2,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=lr\n",
    ")\n",
    "# model = model.to('cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dm.test_dataset, batch_size=128, shuffle=True)\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "n_batches = 20  # Number of batches to process\n",
    "# Run inference on the test data and print progress\n",
    "with torch.no_grad():\n",
    "    total_batches = len(test_loader)\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= n_batches:  # Stop after 20 batches\n",
    "            break\n",
    "        print(f\"Processing batch {i + 1} out of {total_batches}...\")\n",
    "        images, targets = batch\n",
    "        images = images.to(device)\n",
    "        preds = model(images)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "# Concatenate all predictions and targets\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Convert model outputs to predicted class indices\n",
    "pred_labels = np.argmax(all_preds, axis=1)\n",
    "\n",
    "# Compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(all_targets, pred_labels)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "cm = confusion_matrix(all_targets, pred_labels, normalize='true')\n",
    "class_labels = list(class_to_idx.keys())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e9787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eed9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2bdec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.3 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8658764d9b797a2c8f9923ddcd38c86560d2e4c4233111378203e5da49e50175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
