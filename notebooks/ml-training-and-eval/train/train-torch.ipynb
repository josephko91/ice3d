{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Joseph Ko <br>\n",
    "Reproducible notebook to train pytorch models from: \"A Machine Learning Framework for Predicting Microphysical Properties of Ice Crystals from Cloud Particle Imagery\" (Ko et al. 2025) <br>\n",
    "Required packages: see torch.yaml file for required files\n",
    "Models were trained using NVIDIA a100 GPUs. Package configurations may vary depending on the GPU you use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f86477af-3c28-415d-a7e9-6bd3bddaa708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import torchvision.transforms as T\n",
    "import json\n",
    "# Add your project root to sys.path for imports \n",
    "# (models and data modules should be in this directory)\n",
    "sys.path.append('/home/jko/ice3d')\n",
    "# Import your models and datamodules\n",
    "from models.mlp_regression import MLPRegression\n",
    "from models.mlp_classification import MLPClassification\n",
    "from models.cnn_regression import VanillaCNNRegression\n",
    "from models.cnn_classification import VanillaCNNClassification\n",
    "from models.resnet18_regression import ResNet18Regression\n",
    "from models.resnet18_classification import ResNet18Classification\n",
    "from data.single_view_datamodule import SingleViewDataModule\n",
    "from data.stereo_view_datamodule import StereoViewDataModule\n",
    "from data.tabular_datamodule import TabularDataModule\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configurations and global paths\n",
    "targets_reg = 'rho_eff,sa_eff' # regression targets\n",
    "targets_cls = 'n_arms' # classification targets\n",
    "tabular_root = '/home/jko/synth-ros-data/tabular-data-v2/shuffled_small'\n",
    "h5_root = '/home/jko/synth-ros-data/imgs-ml-ready/shuffled_small'\n",
    "tabular_path = os.path.join(tabular_root, 'ros-tabular-data-shuffled-default-subset-700000.parquet')\n",
    "tabular_2ds_path = os.path.join(tabular_root, 'ros-tabular-data-stereo-default-2ds-shuffled-subset-700000.parquet')\n",
    "tabular_phips_path = os.path.join(tabular_root, 'ros-tabular-data-stereo-default-phips-shuffled-subset-700000.parquet')\n",
    "h5_default_path = os.path.join(h5_root, 'default_shuffled_small.h5')\n",
    "h5_2ds_path = os.path.join(h5_root, '2ds_shuffled_small.h5')\n",
    "h5_phips_path = os.path.join(h5_root, 'phips_shuffled_small.h5')\n",
    "log_dir = './lightning_logs'\n",
    "feature_names='aspect_ratio,aspect_ratio_elip,extreme_pts,contour_area,contour_perimeter,area_ratio,complexity,circularity'\n",
    "train_idx=\"/home/jko/synth-ros-data/idx/idx-train-sequential-subset-700k.txt\"\n",
    "val_idx=\"/home/jko/synth-ros-data/idx/idx-val-sequential-subset-700k.txt\"\n",
    "test_idx=\"/home/jko/synth-ros-data/idx/idx-test-sequential-subset-700k.txt\"\n",
    "class_to_idx_json='/home/jko/ice3d/data/class_to_idx.json'\n",
    "n_rand = 666 # random seed\n",
    "num_gpus = 1 # set to 1 to prevent issues with multi-gpu training in jupyter environment\n",
    "ncpus = 32 # number of cpus available\n",
    "prefetch_factor = int(ncpus/2)\n",
    "subset_size = 0.1 # set to 10% to speed up training for demonstration purposes\n",
    "batch_size=128\n",
    "lr=1e-3\n",
    "max_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-configure argument lists for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store args of each model in a dictionary\n",
    "args_dict = {} # initialize\n",
    "\n",
    "# MLP, Regression\n",
    "args_dict['mlp_reg'] = SimpleNamespace(\n",
    "    model='mlp_regression',\n",
    "    data_type='tabular',\n",
    "    tabular_file=tabular_path,\n",
    "    targets=targets_reg,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='regression',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    feature_names=feature_names,\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=None\n",
    ")\n",
    "# MLP, Classification\n",
    "args_dict['mlp_cls'] = SimpleNamespace(\n",
    "    model='mlp_regression',\n",
    "    data_type='tabular',\n",
    "    tabular_file=tabular_path,\n",
    "    targets=targets_cls,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='regression',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    class_to_idx_json=class_to_idx_json,\n",
    "    feature_names=feature_names,\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    ")\n",
    "# CNN, Regression\n",
    "args_dict['cnn_reg'] = SimpleNamespace(\n",
    "    model='cnn_regression',\n",
    "    data_type='single_view_h5',\n",
    "    hdf_file=h5_default_path,\n",
    "    targets=targets_reg,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='regression',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=None,\n",
    "    input_channels=1\n",
    ")\n",
    "# CNN, Classification\n",
    "args_dict['cnn_cls'] = SimpleNamespace(\n",
    "    model='cnn_classification',\n",
    "    data_type='single_view_h5',\n",
    "    hdf_file=h5_default_path,\n",
    "    targets=targets_cls,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='classification',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=class_to_idx_json,\n",
    "    input_channels=1\n",
    ")\n",
    "# ResNet-18, Regression, Singlve View\n",
    "args_dict['resnet18_reg'] = SimpleNamespace(\n",
    "    model='resnet18_regression',\n",
    "    data_type='single_view_h5',\n",
    "    hdf_file=h5_default_path,\n",
    "    targets=targets_reg,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='regression',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=None,\n",
    "    input_channels=1\n",
    ")\n",
    "# ResNet-18, Regression, Stereo, 2DS\n",
    "args_dict['resnet_reg_stereo_2ds'] = SimpleNamespace(\n",
    "    model='resnet18_regression',\n",
    "    data_type='stereo_view_h5',\n",
    "    hdf_file_left=h5_default_path,\n",
    "    hdf_file_right=h5_2ds_path,\n",
    "    targets=targets_reg,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='regression',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=None,\n",
    "    input_channels=2\n",
    ")\n",
    "# ResNet-18, Regression, Stereo, PHIPS\n",
    "args_dict['resnet18_reg_stereo_phips'] = SimpleNamespace(\n",
    "    model='resnet18_regression',\n",
    "    data_type='stereo_view_h5',\n",
    "    hdf_file_left=h5_default_path,\n",
    "    hdf_file_right=h5_phips_path,\n",
    "    targets=targets_reg,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='regression',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=None,\n",
    "    input_channels=2\n",
    ")\n",
    "# ResNet-18, Classification, Singlve View\n",
    "args_dict['resnet18_cls'] = SimpleNamespace(\n",
    "    model='resnet18_classification',\n",
    "    data_type='single_view_h5',\n",
    "    hdf_file=h5_default_path,\n",
    "    targets=targets_cls,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='classification',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=class_to_idx_json,\n",
    "    input_channels=1\n",
    ")\n",
    "# ResNet-18, Classification, Stereo, 2DS\n",
    "args_dict['resnet18_cls_stereo_2ds'] = SimpleNamespace(\n",
    "    model='resnet18_regression',\n",
    "    data_type='stereo_view_h5',\n",
    "    hdf_file_left=h5_default_path,\n",
    "    hdf_file_right=h5_2ds_path,\n",
    "    targets=targets_cls,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='classification',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=class_to_idx_json,\n",
    "    input_channels=2\n",
    ")\n",
    "# ResNet-18, Classification, Stereo, PHIPS\n",
    "args_dict['resnet18_cls_stereo_phips'] = SimpleNamespace(\n",
    "    model='resnet18_regression',\n",
    "    data_type='stereo_view_h5',\n",
    "    hdf_file_left=h5_default_path,\n",
    "    hdf_file_right=h5_phips_path,\n",
    "    targets=targets_cls,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    max_epochs=max_epochs,\n",
    "    subset_size=subset_size,\n",
    "    seed=n_rand,\n",
    "    num_workers=ncpus,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    task_type='classification',\n",
    "    log_dir=log_dir,\n",
    "    tb_log_name='tb',\n",
    "    csv_log_name='csv',\n",
    "    num_gpus=num_gpus,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    test_idx=test_idx,\n",
    "    class_to_idx_json=class_to_idx_json,\n",
    "    input_channels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args, input_size=None, output_size=None, num_classes=None):\n",
    "    if args.model == 'mlp_regression':\n",
    "        return MLPRegression(input_size, output_size, learning_rate=args.lr)\n",
    "    elif args.model == 'mlp_classification':\n",
    "        return MLPClassification(input_size, num_classes, learning_rate=args.lr)\n",
    "    elif args.model == 'cnn_regression':\n",
    "        return VanillaCNNRegression(input_channels=args.input_channels, output_size=output_size, learning_rate=args.lr)\n",
    "    elif args.model == 'cnn_classification':\n",
    "        return VanillaCNNClassification(input_channels=args.input_channels, num_classes=num_classes, learning_rate=args.lr)\n",
    "    elif args.model == 'resnet18_regression':\n",
    "        return ResNet18Regression(input_channels=args.input_channels, output_size=output_size, learning_rate=args.lr)\n",
    "    elif args.model == 'resnet18_classification':\n",
    "        return ResNet18Classification(input_channels=args.input_channels, num_classes=num_classes, learning_rate=args.lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {args.model}\")\n",
    "\n",
    "def get_transforms(args):\n",
    "    transforms = {}\n",
    "    # Define transforms based on data_type\n",
    "    if args.data_type in ['single_view_h5', 'stereo_view_h5']:\n",
    "        train_transform = T.Compose([\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomVerticalFlip(),\n",
    "                T.Normalize(mean=[0.5] * args.input_channels, std=[1.0] * args.input_channels)\n",
    "            ])\n",
    "        val_transform = T.Compose([\n",
    "                T.Normalize(mean=[0.5] * args.input_channels, std=[1.0] * args.input_channels)\n",
    "            ])\n",
    "        transforms['train'] = train_transform\n",
    "        transforms['val'] = val_transform\n",
    "        transforms['test'] = val_transform\n",
    "        # define target transform\n",
    "        if args.task_type == 'classification':\n",
    "            target_transform = None\n",
    "        else:\n",
    "            def log_transform(x):\n",
    "                return torch.log(x)\n",
    "            target_transform = log_transform\n",
    "        transforms['train_target'] = target_transform\n",
    "        transforms['val_target'] = target_transform\n",
    "        transforms['test_target'] = target_transform    \n",
    "        return transforms\n",
    "    elif args.data_type == 'tabular':\n",
    "        # define target transform\n",
    "        if args.task_type == 'classification':\n",
    "            target_transform = None\n",
    "        else:\n",
    "            def log_transform(x):\n",
    "                return torch.log(x)\n",
    "            target_transform = log_transform\n",
    "        transforms['target'] = target_transform\n",
    "        return transforms\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_datamodule(args, class_to_idx=None):\n",
    "    transforms = get_transforms(args)\n",
    "    if args.data_type == 'single_view_h5':\n",
    "        return SingleViewDataModule(\n",
    "            hdf_file=args.hdf_file,\n",
    "            target_names=args.targets.split(','),\n",
    "            train_idx=None,\n",
    "            val_idx=None,\n",
    "            test_idx=None,\n",
    "            batch_size=args.batch_size,\n",
    "            subset_size=args.subset_size,\n",
    "            subset_seed=args.seed,\n",
    "            num_workers=args.num_workers,\n",
    "            prefetch_factor=args.prefetch_factor,\n",
    "            train_transform=transforms['train'],\n",
    "            val_transform=transforms['val'],\n",
    "            test_transform=transforms['test'],\n",
    "            train_target_transform=transforms['train_target'],\n",
    "            val_target_transform=transforms['val_target'],\n",
    "            test_target_transform=transforms['test_target'],\n",
    "            task_type=args.task_type,\n",
    "            class_to_idx=class_to_idx\n",
    "        )\n",
    "    elif args.data_type == 'stereo_view_h5':\n",
    "        return StereoViewDataModule(\n",
    "            hdf_file_left=args.hdf_file_left,\n",
    "            hdf_file_right=args.hdf_file_right,\n",
    "            target_names=args.targets.split(','),\n",
    "            train_idx=None,\n",
    "            val_idx=None,\n",
    "            test_idx=None,\n",
    "            batch_size=args.batch_size,\n",
    "            subset_size=args.subset_size,\n",
    "            subset_seed=args.seed,\n",
    "            num_workers=args.num_workers,\n",
    "            prefetch_factor=args.prefetch_factor,\n",
    "            train_transform=transforms['train'],\n",
    "            val_transform=transforms['val'],\n",
    "            test_transform=transforms['test'],\n",
    "            train_target_transform=transforms['train_target'],\n",
    "            val_target_transform=transforms['val_target'],\n",
    "            test_target_transform=transforms['test_target'],\n",
    "            task_type=args.task_type,\n",
    "            class_to_idx=class_to_idx\n",
    "        )\n",
    "    elif args.data_type == 'tabular':\n",
    "        feature_names = args.feature_names.split(',') if args.feature_names else None\n",
    "        return TabularDataModule(\n",
    "            data_file=args.tabular_file,\n",
    "            feature_names=feature_names,\n",
    "            target_names=args.targets.split(','),\n",
    "            batch_size=args.batch_size,\n",
    "            subset_size=args.subset_size,\n",
    "            subset_seed=args.seed,\n",
    "            num_workers=args.num_workers,\n",
    "            task_type=args.task_type,\n",
    "            class_to_idx=class_to_idx,\n",
    "            target_transform=transforms['target'],\n",
    "            train_idx=args.train_idx,\n",
    "            val_idx=args.val_idx,   \n",
    "            test_idx=args.test_idx\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data type: {args.data_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get arguments\n",
    "args = args_dict['mlp_reg']\n",
    "# Load class_to_idx mapping if provided\n",
    "class_to_idx = None\n",
    "if args.class_to_idx_json is not None:\n",
    "    with open(args.class_to_idx_json, 'r') as f:\n",
    "        class_to_idx = json.load(f)\n",
    "# Ensure log directory exists\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "tb_logger = TensorBoardLogger(args.log_dir, name=args.tb_log_name)\n",
    "csv_logger = CSVLogger(args.log_dir, name=args.csv_log_name)\n",
    "\n",
    "dm = get_datamodule(args, class_to_idx=class_to_idx)\n",
    "dm.setup()\n",
    "input_size = None\n",
    "output_size = None\n",
    "num_classes = None\n",
    "if args.model.startswith('mlp'):\n",
    "    # For tabular data, infer input/output sizes from datamodule\n",
    "    if args.data_type == 'tabular':\n",
    "        input_size = dm.input_size\n",
    "        if args.task_type == 'regression':\n",
    "            output_size = len(args.targets.split(','))\n",
    "        else:\n",
    "            num_classes = dm.num_classes\n",
    "elif args.model.endswith('classification'):\n",
    "    if class_to_idx is not None:\n",
    "        num_classes = len(class_to_idx)\n",
    "    else: # default to 7 classes\n",
    "        num_classes = 7\n",
    "elif args.model.endswith('regression'):\n",
    "    output_size = len(args.targets.split(','))\n",
    "model = get_model(args, input_size=input_size, output_size=output_size, num_classes=num_classes)\n",
    "# define checkpoint settings\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",         # Metric to monitor\n",
    "    mode=\"min\",                 # Save checkpoints with lower val_loss\n",
    "    save_top_k=3,               # Save the 3 best models\n",
    "    filename=\"model-{epoch:02d}-{val_loss:.4f}\",  # Custom filename\n",
    "    # every_n_epochs=1,           # Save every epoch (optional)\n",
    "    save_last=True              # Also save the last epoch\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=args.max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=num_gpus,\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get arguments\n",
    "args = args_dict['mlp_cls']\n",
    "# Load class_to_idx mapping if provided\n",
    "class_to_idx = None\n",
    "if args.class_to_idx_json is not None:\n",
    "    with open(args.class_to_idx_json, 'r') as f:\n",
    "        class_to_idx = json.load(f)\n",
    "# Ensure log directory exists\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "tb_logger = TensorBoardLogger(args.log_dir, name=args.tb_log_name)\n",
    "csv_logger = CSVLogger(args.log_dir, name=args.csv_log_name)\n",
    "\n",
    "dm = get_datamodule(args, class_to_idx=class_to_idx)\n",
    "dm.setup()\n",
    "input_size = None\n",
    "output_size = None\n",
    "num_classes = None\n",
    "if args.model.startswith('mlp'):\n",
    "    # For tabular data, infer input/output sizes from datamodule\n",
    "    if args.data_type == 'tabular':\n",
    "        input_size = dm.input_size\n",
    "        if args.task_type == 'regression':\n",
    "            output_size = len(args.targets.split(','))\n",
    "        else:\n",
    "            num_classes = dm.num_classes\n",
    "elif args.model.endswith('classification'):\n",
    "    if class_to_idx is not None:\n",
    "        num_classes = len(class_to_idx)\n",
    "    else: # default to 7 classes\n",
    "        num_classes = 7\n",
    "elif args.model.endswith('regression'):\n",
    "    output_size = len(args.targets.split(','))\n",
    "model = get_model(args, input_size=input_size, output_size=output_size, num_classes=num_classes)\n",
    "# define checkpoint settings\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",         # Metric to monitor\n",
    "    mode=\"min\",                 # Save checkpoints with lower val_loss\n",
    "    save_top_k=3,               # Save the 3 best models\n",
    "    filename=\"model-{epoch:02d}-{val_loss:.4f}\",  # Custom filename\n",
    "    # every_n_epochs=1,           # Save every epoch (optional)\n",
    "    save_last=True              # Also save the last epoch\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=args.max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=num_gpus,\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | conv1 | Conv2d    | 160    | train\n",
      "1 | conv2 | Conv2d    | 4.6 K  | train\n",
      "2 | conv3 | Conv2d    | 18.5 K | train\n",
      "3 | pool  | MaxPool2d | 0      | train\n",
      "4 | fc1   | Linear    | 6.4 M  | train\n",
      "5 | fc2   | Linear    | 8.3 K  | train\n",
      "6 | fc3   | Linear    | 130    | train\n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.817    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06c85d7665e460d8e3ee13427964573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b5b710c47543f1949b913749c65c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0600da35371446d85c820da7b868e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acfac764afc430cb9c9803a5c562b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8103e9d15b4ba59e915b7163e83f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeac2ce916324ff3afd19fb26ad4713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316498caed0949c8b344212e8023b797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# get arguments\n",
    "args = args_dict['cnn_reg']\n",
    "# Load class_to_idx mapping if provided\n",
    "class_to_idx = None\n",
    "if args.class_to_idx_json is not None:\n",
    "    with open(args.class_to_idx_json, 'r') as f:\n",
    "        class_to_idx = json.load(f)\n",
    "# Ensure log directory exists\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "tb_logger = TensorBoardLogger(args.log_dir, name=args.tb_log_name)\n",
    "csv_logger = CSVLogger(args.log_dir, name=args.csv_log_name)\n",
    "\n",
    "dm = get_datamodule(args, class_to_idx=class_to_idx)\n",
    "dm.setup()\n",
    "input_size = None\n",
    "output_size = None\n",
    "num_classes = None\n",
    "if args.model.startswith('mlp'):\n",
    "    # For tabular data, infer input/output sizes from datamodule\n",
    "    if args.data_type == 'tabular':\n",
    "        input_size = dm.input_size\n",
    "        if args.task_type == 'regression':\n",
    "            output_size = len(args.targets.split(','))\n",
    "        else:\n",
    "            num_classes = dm.num_classes\n",
    "elif args.model.endswith('classification'):\n",
    "    if class_to_idx is not None:\n",
    "        num_classes = len(class_to_idx)\n",
    "    else: # default to 7 classes\n",
    "        num_classes = 7\n",
    "elif args.model.endswith('regression'):\n",
    "    output_size = len(args.targets.split(','))\n",
    "model = get_model(args, input_size=input_size, output_size=output_size, num_classes=num_classes)\n",
    "# define checkpoint settings\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",         # Metric to monitor\n",
    "    mode=\"min\",                 # Save checkpoints with lower val_loss\n",
    "    save_top_k=3,               # Save the 3 best models\n",
    "    filename=\"model-{epoch:02d}-{val_loss:.4f}\",  # Custom filename\n",
    "    # every_n_epochs=1,           # Save every epoch (optional)\n",
    "    save_last=True              # Also save the last epoch\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=args.max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=num_gpus,\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | conv1 | Conv2d    | 160    | train\n",
      "1 | conv2 | Conv2d    | 4.6 K  | train\n",
      "2 | conv3 | Conv2d    | 18.5 K | train\n",
      "3 | pool  | MaxPool2d | 0      | train\n",
      "4 | fc1   | Linear    | 6.4 M  | train\n",
      "5 | fc2   | Linear    | 8.3 K  | train\n",
      "6 | fc3   | Linear    | 455    | train\n",
      "--------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.819    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce8af4ec0b5412c92fdc196ab479ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48711efccbb34461adaf741f90e33c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117a224ea8c74ea1868817274f1d7e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda941ba8b6b45f08fbf4058cdf9098f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f11c92f27b049e293d0146419066281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfd4b8d97064e18ab09ea87bfa7d937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf38b7179664c5aad2f43cd8f324786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# get arguments\n",
    "args = args_dict['cnn_cls']\n",
    "# Load class_to_idx mapping if provided\n",
    "class_to_idx = None\n",
    "if args.class_to_idx_json is not None:\n",
    "    with open(args.class_to_idx_json, 'r') as f:\n",
    "        class_to_idx = json.load(f)\n",
    "# Ensure log directory exists\n",
    "os.makedirs(args.log_dir, exist_ok=True)\n",
    "tb_logger = TensorBoardLogger(args.log_dir, name=args.tb_log_name)\n",
    "csv_logger = CSVLogger(args.log_dir, name=args.csv_log_name)\n",
    "\n",
    "dm = get_datamodule(args, class_to_idx=class_to_idx)\n",
    "dm.setup()\n",
    "input_size = None\n",
    "output_size = None\n",
    "num_classes = None\n",
    "if args.model.startswith('mlp'):\n",
    "    # For tabular data, infer input/output sizes from datamodule\n",
    "    if args.data_type == 'tabular':\n",
    "        input_size = dm.input_size\n",
    "        if args.task_type == 'regression':\n",
    "            output_size = len(args.targets.split(','))\n",
    "        else:\n",
    "            num_classes = dm.num_classes\n",
    "elif args.model.endswith('classification'):\n",
    "    if class_to_idx is not None:\n",
    "        num_classes = len(class_to_idx)\n",
    "    else: # default to 7 classes\n",
    "        num_classes = 7\n",
    "elif args.model.endswith('regression'):\n",
    "    output_size = len(args.targets.split(','))\n",
    "model = get_model(args, input_size=input_size, output_size=output_size, num_classes=num_classes)\n",
    "# define checkpoint settings\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",         # Metric to monitor\n",
    "    mode=\"min\",                 # Save checkpoints with lower val_loss\n",
    "    save_top_k=3,               # Save the 3 best models\n",
    "    filename=\"model-{epoch:02d}-{val_loss:.4f}\",  # Custom filename\n",
    "    # every_n_epochs=1,           # Save every epoch (optional)\n",
    "    save_last=True              # Also save the last epoch\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=args.max_epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=num_gpus,\n",
    "    logger=[csv_logger, tb_logger],\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression, Single View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression, Stereo, 2DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression, Stereo, PHIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification, Single View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification, Stereo, 2DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification, Stereo, PHIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.3 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8658764d9b797a2c8f9923ddcd38c86560d2e4c4233111378203e5da49e50175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
